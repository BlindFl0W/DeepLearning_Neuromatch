{"metadata":{"colab":{"collapsed_sections":[],"include_colab_link":true,"name":"W3D5_Tutorial2","provenance":[],"toc_visible":true},"kernel":{"display_name":"Python 3","language":"python","name":"python3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D5_ReinforcementLearningForGamesAndDlThinking3/student/W3D5_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D5_ReinforcementLearningForGamesAndDlThinking3/student/W3D5_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>","metadata":{"colab_type":"text","execution":{},"id":"view-in-github"}},{"cell_type":"markdown","source":"# Tutorial 2: Deep Learning Thinking 3\n\n**Week 3, Day 5: Reinforcement Learning for Games & DL Thinking 3**\n\n**By Neuromatch Academy**\n\n__Content creators:__ Konrad Kording, Lyle Ungar\n\n__Content reviewers:__ Ella Batty, Shaonan Wang, Gunnar Blohm\n\n__Content editors:__ Ella Batty, Shaonan Wang\n\n__Production editors:__ Ella Batty, Spiros Chavlis","metadata":{"execution":{}}},{"cell_type":"markdown","source":"---\n# Tutorial Objectives\n\n","metadata":{"execution":{}}},{"cell_type":"code","source":"# @title Tutorial slides\nfrom IPython.display import IFrame\nlink_id = \"2f7ay\"\nprint(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\nIFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:30.985587Z","iopub.execute_input":"2024-08-03T14:49:30.986177Z","iopub.status.idle":"2024-08-03T14:49:31.024140Z","shell.execute_reply.started":"2024-08-03T14:49:30.986126Z","shell.execute_reply":"2024-08-03T14:49:31.023077Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"If you want to download the slides: https://osf.io/download/2f7ay/\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<IPython.lib.display.IFrame at 0x7a0a63a79810>","text/html":"\n        <iframe\n            width=\"854\"\n            height=\"480\"\n            src=\"https://mfr.ca-1.osf.io/render?url=https://osf.io/2f7ay/?direct%26mode=render%26action=download%26mode=render\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "},"metadata":{}}]},{"cell_type":"markdown","source":"---\n# Setup","metadata":{"execution":{}}},{"cell_type":"code","source":"# @title Install and import feedback gadget\n\n!pip3 install vibecheck datatops --quiet\n\nfrom vibecheck import DatatopsContentReviewContainer\ndef content_review(notebook_section: str):\n    return DatatopsContentReviewContainer(\n        \"\",  # No text prompt\n        notebook_section,\n        {\n            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n            \"name\": \"neuromatch_dl\",\n            \"user_key\": \"f379rz8y\",\n        },\n    ).render()\n\n\nfeedback_prefix = \"W3D5_T2\"","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:31.026011Z","iopub.execute_input":"2024-08-03T14:49:31.026334Z","iopub.status.idle":"2024-08-03T14:49:53.414325Z","shell.execute_reply.started":"2024-08-03T14:49:31.026306Z","shell.execute_reply":"2024-08-03T14:49:53.413227Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"---\n# Section 1: Intro to Deep Learning Thinking 3\n\n*Time estimate: ~3 mins*","metadata":{"execution":{}}},{"cell_type":"markdown","source":"This tutorial is the third installment of our deep learning thinking series. Like the others, there will be no coding! Instead, you will watch a series of vignettes about various scenarios where you want to use a neural network.\n\n\nEach section below will start with a vignette where either Lyle or Konrad is trying to figure out how to set up a neural network for a specific problem. Try to think of questions you want to ask them as you watch, then pay attention to what questions Lyle and Konrad are asking. Were they what you would have asked? How do their questions help quickly clarify the situation?","metadata":{"execution":{}}},{"cell_type":"code","source":"# @title Video 1: Intro to DL Thinking 3\nfrom ipywidgets import widgets\nfrom IPython.display import YouTubeVideo\nfrom IPython.display import IFrame\nfrom IPython.display import display\n\n\nclass PlayVideo(IFrame):\n  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n    self.id = id\n    if source == 'Bilibili':\n      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n    elif source == 'Osf':\n      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n\n\ndef display_videos(video_ids, W=400, H=300, fs=1):\n  tab_contents = []\n  for i, video_id in enumerate(video_ids):\n    out = widgets.Output()\n    with out:\n      if video_ids[i][0] == 'Youtube':\n        video = YouTubeVideo(id=video_ids[i][1], width=W,\n                             height=H, fs=fs, rel=0)\n        print(f'Video available at https://youtube.com/watch?v={video.id}')\n      else:\n        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n                          height=H, fs=fs, autoplay=False)\n        if video_ids[i][0] == 'Bilibili':\n          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n        elif video_ids[i][0] == 'Osf':\n          print(f'Video available at https://osf.io/{video.id}')\n      display(video)\n    tab_contents.append(out)\n  return tab_contents\n\n\nvideo_ids = [('Youtube', '_tGTv3FUBZ4'), ('Bilibili', 'BV11V411g7dm')]\ntab_contents = display_videos(video_ids, W=854, H=480)\ntabs = widgets.Tab()\ntabs.children = tab_contents\nfor i in range(len(tab_contents)):\n  tabs.set_title(i, video_ids[i][0])\ndisplay(tabs)","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:53.416011Z","iopub.execute_input":"2024-08-03T14:49:53.416426Z","iopub.status.idle":"2024-08-03T14:49:53.517773Z","shell.execute_reply.started":"2024-08-03T14:49:53.416384Z","shell.execute_reply":"2024-08-03T14:49:53.516639Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48c4587c6f324318be34ddd8c97cd907"}},"metadata":{}}]},{"cell_type":"code","source":"# @title Submit your feedback\ncontent_review(f\"{feedback_prefix}_Intro_to_DL_Thinking_3_Video\")","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:53.520921Z","iopub.execute_input":"2024-08-03T14:49:53.521318Z","iopub.status.idle":"2024-08-03T14:49:53.563179Z","shell.execute_reply.started":"2024-08-03T14:49:53.521282Z","shell.execute_reply":"2024-08-03T14:49:53.562008Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(VBox(children=(HBox(children=(Button(description='ðŸ™‚', layout=Layout(height='auto', padding='0.5â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d118857a967b474aad2b1dd6941538b7"}},"metadata":{}}]},{"cell_type":"markdown","source":"---\n# Section 2: The Future\n\n*Time estimate: ~15 mins*","metadata":{"execution":{}}},{"cell_type":"code","source":"# @title Video 2: The Future Vignette\nfrom ipywidgets import widgets\nfrom IPython.display import YouTubeVideo\nfrom IPython.display import IFrame\nfrom IPython.display import display\n\n\nclass PlayVideo(IFrame):\n  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n    self.id = id\n    if source == 'Bilibili':\n      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n    elif source == 'Osf':\n      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n\n\ndef display_videos(video_ids, W=400, H=300, fs=1):\n  tab_contents = []\n  for i, video_id in enumerate(video_ids):\n    out = widgets.Output()\n    with out:\n      if video_ids[i][0] == 'Youtube':\n        video = YouTubeVideo(id=video_ids[i][1], width=W,\n                             height=H, fs=fs, rel=0)\n        print(f'Video available at https://youtube.com/watch?v={video.id}')\n      else:\n        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n                          height=H, fs=fs, autoplay=False)\n        if video_ids[i][0] == 'Bilibili':\n          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n        elif video_ids[i][0] == 'Osf':\n          print(f'Video available at https://osf.io/{video.id}')\n      display(video)\n    tab_contents.append(out)\n  return tab_contents\n\n\nvideo_ids = [('Youtube', 'iHUZ0FJH5yM'), ('Bilibili', 'BV1fX4y1n747')]\ntab_contents = display_videos(video_ids, W=854, H=480)\ntabs = widgets.Tab()\ntabs.children = tab_contents\nfor i in range(len(tab_contents)):\n  tabs.set_title(i, video_ids[i][0])\ndisplay(tabs)","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:53.565227Z","iopub.execute_input":"2024-08-03T14:49:53.565621Z","iopub.status.idle":"2024-08-03T14:49:53.674303Z","shell.execute_reply.started":"2024-08-03T14:49:53.565584Z","shell.execute_reply":"2024-08-03T14:49:53.673239Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd16b351c4aa48f085deb42adf674d86"}},"metadata":{}}]},{"cell_type":"code","source":"# @title Submit your feedback\ncontent_review(f\"{feedback_prefix}_The_Future_Video\")","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:53.675835Z","iopub.execute_input":"2024-08-03T14:49:53.676232Z","iopub.status.idle":"2024-08-03T14:49:53.718634Z","shell.execute_reply.started":"2024-08-03T14:49:53.676193Z","shell.execute_reply":"2024-08-03T14:49:53.717456Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(VBox(children=(HBox(children=(Button(description='ðŸ™‚', layout=Layout(height='auto', padding='0.5â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5acef6ad5cf04829990ea9ef01f3108d"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Think! 1: The Future\n\nThings change over time! How might we address this problem in machine learning?\n\nPlease discuss as a group. If you get stuck, you can uncover the hints below one at a time to help guide your discussion. Please spend some time discussing before uncovering the next hint though!","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 1 </font></summary>\n\nCan you think of a scenario in your life where data is i.i.d?\n\nLet's use an example. Konrad hears questions x about a topic during a class he's teaching, and tries to learn answers y to each of them. Will the questions next year be the same as the questions this year?\n\nOnce we know that the questions next year will be different, what should Konrad do to be ready for the questions next year?\n\nMore generally, why does it matter that the future is different from the past? How does that relate to i.i.d.?\n","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 2 </font></summary>\n\nSo the future will have a different distribution p(X) than the past. It will also have a different distribution of good answers p(Y). After all, deep learning is evolving. And training in deep learning is also evolving. What could be a good model, in the context of deep learning, of p(X)? How could it be built into a deep learning system? Would it even be necessary to build it in? What about p(Y)? Would that need to be built in? Arguably, DL is about how to find p(Y|X). Why do we need to care about X changing? Why about Y changing?\n\nIn this context, let us talk about curiosity. If p(X) and p(Y) are both changing, what would we want to learn about? We probably want to focus on things that will still be true in the future. What are the things that are true today and will be true in the future? Which things change?\n","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 3 </font></summary>\n\nThe objects in a room change. Their affordances do not, you will be able to sit on a chair tomorrow. The causal relations in the world do not change. For example, fire will still burn wood in the future. Their constituents do not change. For example, mammals still consist of blood, flesh, and bones. Other things change all the time. Who is in this room? Who am I talking with? There are also things that do change that I do not care about, e.g. somewhere far away there is a causal system I do not understand. So we do not just want to have good models in the future, we want to have good models to answer the kinds of questions we will actually be asked.\n","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for solution  </font></summary>\n\nHere is a paper talking about these phenomena: https://arxiv.org/abs/2201.07372.\n\nThe paper above is worth a full read! In brief, it outlines an argument that most AI focuses on *retrospective learning*, where it uses past experiences to learn and make predictions in the future, assuming the future mimics the past. The authors argue that true intelligence is *prospective learning*, where AI learns for an uncertain future by updating internal models to be useful for future novel tasks. They articulate four revelant factors that jointly define prospective learning: \"**Continual learning** enables intelligences to remember those aspects of the past which\nit believes will be most useful in the future. Prospective **constraints** (including biases and priors) facilitate the\nintelligence finding general solutions that will be applicable to future problems. **Curiosity** motivates taking actions\nthat inform future decision making, including in previously unmet situations. **Causal estimation** enables learning\nthe structure of relations that guide choosing actions for specific outcomes, even when the specific action-outcome\ncontingencies have never been observed before.\"\n\nDiscussion point: in the light of this paper, how would you design DL algorithms differently? Which problems are easy vs hard to overcome?\n","metadata":{"execution":{}}},{"cell_type":"code","source":"# @title Submit your feedback\ncontent_review(f\"{feedback_prefix}_The_Future_Discussion\")","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:53.720307Z","iopub.execute_input":"2024-08-03T14:49:53.720816Z","iopub.status.idle":"2024-08-03T14:49:53.769483Z","shell.execute_reply.started":"2024-08-03T14:49:53.720776Z","shell.execute_reply":"2024-08-03T14:49:53.768294Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(VBox(children=(HBox(children=(Button(description='ðŸ™‚', layout=Layout(height='auto', padding='0.5â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb0907611bd74e61af75412fd853378e"}},"metadata":{}}]},{"cell_type":"markdown","source":"---\n# Section 3: In-context Learning\n\n*Time estimate: ~15 mins*","metadata":{"execution":{}}},{"cell_type":"code","source":"# @title Video 3: In-context Learning Vignette\nfrom ipywidgets import widgets\nfrom IPython.display import YouTubeVideo\nfrom IPython.display import IFrame\nfrom IPython.display import display\n\n\nclass PlayVideo(IFrame):\n  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n    self.id = id\n    if source == 'Bilibili':\n      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n    elif source == 'Osf':\n      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n\n\ndef display_videos(video_ids, W=400, H=300, fs=1):\n  tab_contents = []\n  for i, video_id in enumerate(video_ids):\n    out = widgets.Output()\n    with out:\n      if video_ids[i][0] == 'Youtube':\n        video = YouTubeVideo(id=video_ids[i][1], width=W,\n                             height=H, fs=fs, rel=0)\n        print(f'Video available at https://youtube.com/watch?v={video.id}')\n      else:\n        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n                          height=H, fs=fs, autoplay=False)\n        if video_ids[i][0] == 'Bilibili':\n          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n        elif video_ids[i][0] == 'Osf':\n          print(f'Video available at https://osf.io/{video.id}')\n      display(video)\n    tab_contents.append(out)\n  return tab_contents\n\n\nvideo_ids = [('Youtube', 'MQLVLBG-6Rk'), ('Bilibili', 'BV1nz4y1n7J5')]\ntab_contents = display_videos(video_ids, W=854, H=480)\ntabs = widgets.Tab()\ntabs.children = tab_contents\nfor i in range(len(tab_contents)):\n  tabs.set_title(i, video_ids[i][0])\ndisplay(tabs)","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:53.771056Z","iopub.execute_input":"2024-08-03T14:49:53.771469Z","iopub.status.idle":"2024-08-03T14:49:53.872531Z","shell.execute_reply.started":"2024-08-03T14:49:53.771432Z","shell.execute_reply":"2024-08-03T14:49:53.871429Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56eb00d8fef043f19125204d7e6a65cf"}},"metadata":{}}]},{"cell_type":"code","source":"# @title Submit your feedback\ncontent_review(f\"{feedback_prefix}_In_context_learning_Vignette_Video\")","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:53.874144Z","iopub.execute_input":"2024-08-03T14:49:53.874522Z","iopub.status.idle":"2024-08-03T14:49:53.916171Z","shell.execute_reply.started":"2024-08-03T14:49:53.874487Z","shell.execute_reply":"2024-08-03T14:49:53.915058Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(VBox(children=(HBox(children=(Button(description='ðŸ™‚', layout=Layout(height='auto', padding='0.5â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93963901834746b19f03b5402ade6af0"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Think! 2: In-context Learning\n\nPlease discuss as a group. If you get stuck, you can uncover the hints below one at a time to guide your discussion. Please spend some time discussing before uncovering the next hint though!","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 1 </font></summary>\n\nWhat is in context learning? The context is basically the words that LLMs have in their context right now. To predicts what is next we, arguably, solve a machine learning problem. How would you write this as an equation?\n\nTry if you can get chatGPT to solve in-context learning problems.\n","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 2 </font></summary>\n\nLet us talk about the nature of the problem. How to we train systems and how may that relate to how we do in-context learning?\n\nMeta-learning is defined as a system that learns how to learn. Is in-context learning a version of meta-learning?\n","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 3 </font></summary>\n\nHow could the transformer architecture be helpful for solving in-context learning problems? DL does do gradient descent. Do you think there is gradient descent learning in-context?\n","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for solution  </font></summary>\n\nHere are two papers about these topics:\n1. [arxiv:2211.15561](https://arxiv.org/abs/2211.15661)\n2. [arxiv:2212.10559](https://arxiv.org/abs/2212.10559v2)\n\nHere is a summary of the above papers:\n\nPaper 1: Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. The authors investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context.\n\nPaper 2: Large pretrained language models possess impressive In-Context Learning (ICL) capabilities, enabling them to predict labels for unseen inputs with just a few demonstration input-label pairs and without additional parameter updates. However, the underlying mechanism of ICL remains unknown. This paper views language models as meta-optimizers and conceptualizes ICL as a form of implicit finetuning, uncovering that Transformer attention follows a dual form of gradient descent optimization. Experimental comparisons confirm that ICL performs similarly to explicit finetuning in terms of prediction, representation, and attention behavior, and the introduction of momentum-based attention, inspired by meta-optimization, demonstrates its potential for future model design.\n\nDiscussion point: In the light of these papers, how can we build more solid theories of ICL? What is currently understood? What kind of theorems do you think we may be able to prove about ICL? Can we expect to improve ICL through architecture engineering?\n\n\n","metadata":{"execution":{}}},{"cell_type":"code","source":"# @title Submit your feedback\ncontent_review(f\"{feedback_prefix}_In_context_learning_Discussion\")","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T15:41:26.830123Z","iopub.execute_input":"2024-08-03T15:41:26.831222Z","iopub.status.idle":"2024-08-03T15:41:26.872275Z","shell.execute_reply.started":"2024-08-03T15:41:26.831182Z","shell.execute_reply":"2024-08-03T15:41:26.871171Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(VBox(children=(HBox(children=(Button(description='ðŸ™‚', layout=Layout(height='auto', padding='0.5â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aa56b2a49a0470891ffda49b6ef21ce"}},"metadata":{}}]},{"cell_type":"markdown","source":"---\n# Section 4: Memories\n\n\n*Time estimate: ~15 mins*","metadata":{"execution":{}}},{"cell_type":"code","source":"# @title Video 4: Memories Vignette\nfrom ipywidgets import widgets\nfrom IPython.display import YouTubeVideo\nfrom IPython.display import IFrame\nfrom IPython.display import display\n\n\nclass PlayVideo(IFrame):\n  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n    self.id = id\n    if source == 'Bilibili':\n      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n    elif source == 'Osf':\n      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n\n\ndef display_videos(video_ids, W=400, H=300, fs=1):\n  tab_contents = []\n  for i, video_id in enumerate(video_ids):\n    out = widgets.Output()\n    with out:\n      if video_ids[i][0] == 'Youtube':\n        video = YouTubeVideo(id=video_ids[i][1], width=W,\n                             height=H, fs=fs, rel=0)\n        print(f'Video available at https://youtube.com/watch?v={video.id}')\n      else:\n        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n                          height=H, fs=fs, autoplay=False)\n        if video_ids[i][0] == 'Bilibili':\n          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n        elif video_ids[i][0] == 'Osf':\n          print(f'Video available at https://osf.io/{video.id}')\n      display(video)\n    tab_contents.append(out)\n  return tab_contents\n\n\nvideo_ids = [('Youtube', 'SDJv_hnRrl0'), ('Bilibili', 'BV1fs4y1r7Je')]\ntab_contents = display_videos(video_ids, W=854, H=480)\ntabs = widgets.Tab()\ntabs.children = tab_contents\nfor i in range(len(tab_contents)):\n  tabs.set_title(i, video_ids[i][0])\ndisplay(tabs)","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:53.973961Z","iopub.execute_input":"2024-08-03T14:49:53.974342Z","iopub.status.idle":"2024-08-03T14:49:54.083187Z","shell.execute_reply.started":"2024-08-03T14:49:53.974306Z","shell.execute_reply":"2024-08-03T14:49:54.082193Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"970a0b64cab342d390f3855d96b6f161"}},"metadata":{}}]},{"cell_type":"code","source":"# @title Submit your feedback\ncontent_review(f\"{feedback_prefix}_Memories_Vignette_Video\")","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:54.084425Z","iopub.execute_input":"2024-08-03T14:49:54.084738Z","iopub.status.idle":"2024-08-03T14:49:54.124407Z","shell.execute_reply.started":"2024-08-03T14:49:54.084689Z","shell.execute_reply":"2024-08-03T14:49:54.123352Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(VBox(children=(HBox(children=(Button(description='ðŸ™‚', layout=Layout(height='auto', padding='0.5â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a146ee68c0bd4d0ea74459a18d9a886c"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Think! 3: Memories\n\nPlease discuss as a group. If you get stuck, you can uncover the hints below one at a time. Please spend some time discussing before uncovering the next hint, though! You are being real deep learning scientists now, and the answers wonâ€™t be easy.\n\n","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 1 </font></summary>\n\nHow would you define episodic memory?\n\nHow would you define procedural memory?\n\nHow would you define semantic memory?\n","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 2 </font></summary>\n\nThe main interesting thing here is episodic memory. For example, you and your best friend may both know that it is a fact that you had dinner together yesterday night. How would you build episodic memory into a deep learning systems? Can you gradient descent into an episodic memory?\n\n","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 3 </font></summary>\n\nEpisodic memory is after all something about absolute truth. So it is not entirely clear how it relates to gradient descent. It is easy to see how we may use gradient descent for reading though. So what if you instead just had a big blackboard. What would you write on it? When/how would you erase it?","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for solution </font></summary>\n\nHere are a few fun papers.\nhttps://arxiv.org/abs/1410.5401,\nhttps://arxiv.org/abs/1805.07603,\nhttps://arxiv.org/abs/1703.03129\n\nHere is a summary of the above papers:\n\nPaper 1: While modern machine learning has been successful in modeling complex data, it has largely overlooked the use of logical flow control and external memory, despite their importance in computer programs. Recurrent neural networks (RNNs) have the ability to carry out intricate data transformations over extended periods and are Turing-Complete, capable of simulating arbitrary procedures. To simplify the solution of algorithmic tasks, the authors introduce the Neural Turing Machine (NTM), which enriches standard recurrent networks with a large, addressable memory, resembling human working memory and utilizing an attentional process for selective reading and writing. The NTM can be trained through gradient descent, making it a practical mechanism for learning programs.\n\nPaper 2: Deep reinforcement learning (RL) algorithms have seen great advancements by utilizing deep neural networks (DNNs), but they suffer from sample inefficiency. To address this, the authors propose Episodic Memory Deep Q-Networks (EMDQN), a biologically inspired RL algorithm that uses episodic memory for training supervision. Experimental results demonstrate that EMDQN achieves better sample efficiency, outperforming regular DQN and other episodic memory based RL algorithms, requiring only 1/5 of the interactions of DQN to achieve state-of-the-art performance on Atari games.\nPaper 3: Existing memory-augmented deep neural networks face limitations in lifelong and one-shot learning, particularly in remembering rare events. To address this, the authors introduce a large-scale lifelong memory module that utilizes fast nearest-neighbor algorithms for efficiency and scalability. The module is fully differentiable, trained end-to-end without additional supervision, and can be seamlessly integrated into various neural network architectures. Experimental results demonstrate the module's ability to achieve state-of-the-art performance in one-shot learning tasks on the Omniglot dataset and enable lifelong one-shot learning in recurrent neural networks for large-scale machine translation.\n\nDiscussion point: In the light of these papers, which desirable aspects of memory can we already build? How do they differ from the way humans do it? How close are we to the agility of memory exhibited by humans?\n","metadata":{"execution":{}}},{"cell_type":"code","source":"# @title Submit your feedback\ncontent_review(f\"{feedback_prefix}_Memories_Discussion\")","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:54.125669Z","iopub.execute_input":"2024-08-03T14:49:54.126023Z","iopub.status.idle":"2024-08-03T14:49:54.165559Z","shell.execute_reply.started":"2024-08-03T14:49:54.125996Z","shell.execute_reply":"2024-08-03T14:49:54.164536Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(VBox(children=(HBox(children=(Button(description='ðŸ™‚', layout=Layout(height='auto', padding='0.5â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"090d91baef444d0c9603ff3d7f809735"}},"metadata":{}}]},{"cell_type":"markdown","source":"---\n# Section 5: Multiple Information Sources\n\n\n*Time estimate: ~15 mins*","metadata":{"execution":{}}},{"cell_type":"code","source":"# @title Video 5: Multiple Information Sources Vignette\nfrom ipywidgets import widgets\nfrom IPython.display import YouTubeVideo\nfrom IPython.display import IFrame\nfrom IPython.display import display\n\n\nclass PlayVideo(IFrame):\n  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n    self.id = id\n    if source == 'Bilibili':\n      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n    elif source == 'Osf':\n      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n\n\ndef display_videos(video_ids, W=400, H=300, fs=1):\n  tab_contents = []\n  for i, video_id in enumerate(video_ids):\n    out = widgets.Output()\n    with out:\n      if video_ids[i][0] == 'Youtube':\n        video = YouTubeVideo(id=video_ids[i][1], width=W,\n                             height=H, fs=fs, rel=0)\n        print(f'Video available at https://youtube.com/watch?v={video.id}')\n      else:\n        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n                          height=H, fs=fs, autoplay=False)\n        if video_ids[i][0] == 'Bilibili':\n          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n        elif video_ids[i][0] == 'Osf':\n          print(f'Video available at https://osf.io/{video.id}')\n      display(video)\n    tab_contents.append(out)\n  return tab_contents\n\n\nvideo_ids = [('Youtube', '_I_m2n2yz4Q'), ('Bilibili', 'BV1Nh4y1u7FD')]\ntab_contents = display_videos(video_ids, W=854, H=480)\ntabs = widgets.Tab()\ntabs.children = tab_contents\nfor i in range(len(tab_contents)):\n  tabs.set_title(i, video_ids[i][0])\ndisplay(tabs)","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:54.166827Z","iopub.execute_input":"2024-08-03T14:49:54.167102Z","iopub.status.idle":"2024-08-03T14:49:54.259206Z","shell.execute_reply.started":"2024-08-03T14:49:54.167078Z","shell.execute_reply":"2024-08-03T14:49:54.258221Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7775f18f1494a49819597c887c94fcd"}},"metadata":{}}]},{"cell_type":"code","source":"# @title Submit your feedback\ncontent_review(f\"{feedback_prefix}_Multiple_Information_Sources_Vignette_Video\")","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:54.260555Z","iopub.execute_input":"2024-08-03T14:49:54.260912Z","iopub.status.idle":"2024-08-03T14:49:54.299921Z","shell.execute_reply.started":"2024-08-03T14:49:54.260884Z","shell.execute_reply":"2024-08-03T14:49:54.298886Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(VBox(children=(HBox(children=(Button(description='ðŸ™‚', layout=Layout(height='auto', padding='0.5â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c1bfa1f45b84d32845c8d61d3747e94"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Think! 4: Multiple Information Sources\n\nPlease discuss as a group. If you get stuck, you can uncover the hints below one at a time. Please spend some time discussing before uncovering the next hint, though! You are being real deep learning scientists now, and the answers wonâ€™t be easy.","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 1 </font></summary>\n\nThink of a few tasks. If you wanted to solve them, which webpages would you use? How would you combine them? How could you allow a DL system to do the same thing?\n","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 2 </font></summary>\n\nIf you have multiple sources of information, how synergistic are they? How could you incorporate information from multiple sources? Could there be ways of summarizing the things you could get out of a given model?","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 3 </font></summary>\n\nEnglish language and querying can be a way of interacting with such services. Think about webpages as something that you can ask questions to.\n","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for solution </font></summary>\n\nHere are a list of recent relevant projects:\n1. [Auto GPT](https://github.com/Significant-Gravitas/Auto-GPT)\n2. [arxiv:2302.14045](https://arxiv.org/abs/2302.14045)\n3. [PaLMe](https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html)\n\nHere is a summary of the above paper:\n\nA big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Experimental results show that the proposed model achieves impressive performance on language understanding, generation, OCR-free NLP, perception-language tasks, and vision tasks, as well as that MLLMs can benefit from cross-modal transfer.\n\nDiscussion points: How should information be combined across modalities? Why does it help to ccombine them? What does that mean about the future of science?\n","metadata":{"execution":{}}},{"cell_type":"code","source":"# @title Submit your feedback\ncontent_review(f\"{feedback_prefix}_Multiple_Information_Sources_Discussion\")","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:54.301590Z","iopub.execute_input":"2024-08-03T14:49:54.302016Z","iopub.status.idle":"2024-08-03T14:49:54.346184Z","shell.execute_reply.started":"2024-08-03T14:49:54.301971Z","shell.execute_reply":"2024-08-03T14:49:54.344242Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(VBox(children=(HBox(children=(Button(description='ðŸ™‚', layout=Layout(height='auto', padding='0.5â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec159bfe0240458089299bc14d1e0cc4"}},"metadata":{}}]},{"cell_type":"markdown","source":"---\n# Section 6: Language for Robotics\n\n\n*Time estimate: ~15 mins*","metadata":{"execution":{}}},{"cell_type":"code","source":"# @title Video 6: Language for Robotics Vignette\nfrom ipywidgets import widgets\nfrom IPython.display import YouTubeVideo\nfrom IPython.display import IFrame\nfrom IPython.display import display\n\n\nclass PlayVideo(IFrame):\n  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n    self.id = id\n    if source == 'Bilibili':\n      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n    elif source == 'Osf':\n      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n\n\ndef display_videos(video_ids, W=400, H=300, fs=1):\n  tab_contents = []\n  for i, video_id in enumerate(video_ids):\n    out = widgets.Output()\n    with out:\n      if video_ids[i][0] == 'Youtube':\n        video = YouTubeVideo(id=video_ids[i][1], width=W,\n                             height=H, fs=fs, rel=0)\n        print(f'Video available at https://youtube.com/watch?v={video.id}')\n      else:\n        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n                          height=H, fs=fs, autoplay=False)\n        if video_ids[i][0] == 'Bilibili':\n          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n        elif video_ids[i][0] == 'Osf':\n          print(f'Video available at https://osf.io/{video.id}')\n      display(video)\n    tab_contents.append(out)\n  return tab_contents\n\n\nvideo_ids = [('Youtube', 'nLxzECu12IY'), ('Bilibili', 'BV1FF411R7BG')]\ntab_contents = display_videos(video_ids, W=854, H=480)\ntabs = widgets.Tab()\ntabs.children = tab_contents\nfor i in range(len(tab_contents)):\n  tabs.set_title(i, video_ids[i][0])\ndisplay(tabs)","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:54.347354Z","iopub.execute_input":"2024-08-03T14:49:54.347622Z","iopub.status.idle":"2024-08-03T14:49:54.431878Z","shell.execute_reply.started":"2024-08-03T14:49:54.347597Z","shell.execute_reply":"2024-08-03T14:49:54.430918Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Tab(children=(Output(), Output()), _titles={'0': 'Youtube', '1': 'Bilibili'})","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bc14171f9604f1e89ed89b0920a3e99"}},"metadata":{}}]},{"cell_type":"code","source":"# @title Submit your feedback\ncontent_review(f\"{feedback_prefix}_Language_for_Robotics_Video\")","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:54.433009Z","iopub.execute_input":"2024-08-03T14:49:54.433298Z","iopub.status.idle":"2024-08-03T14:49:54.472387Z","shell.execute_reply.started":"2024-08-03T14:49:54.433274Z","shell.execute_reply":"2024-08-03T14:49:54.471291Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(VBox(children=(HBox(children=(Button(description='ðŸ™‚', layout=Layout(height='auto', padding='0.5â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"490e8c9d193c4f1698484571545e2a7e"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Think! 5: Language for Robotics","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 1 </font></summary>\n\nThink about a robotics problem, e.g. emptying the dishwasher. Is there a way of dividing it into subproblems? How many of them? How complex are they?\n","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 2 </font></summary>\n\nIf you look at the subproblems, how complex are they? What could be a good way of describing each subproblem? If a robot controlling AI system would have such a description, how useful would that be?","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for hint 3 </font></summary>\n\nIf we were to use a large language model to interact with the RL systems, how useful would that be? Why is language so useful for this task?","metadata":{"execution":{}}},{"cell_type":"markdown","source":"<details>\n<summary> <font color='green'>Click here for solution </font></summary>\n\nHave a look at this great collection of papers: [Awesome-LLM-Robotocs](https://github.com/GT-RIPL/Awesome-LLM-Robotics)\n\nHere is a summary of the papers listed in github:\nThese papers cover topics such as reasoning, planning, manipulation, instructions and navigation, and simulation frameworks.\nSome of the papers use pre-trained models such as GPT-3, BERT, or CLIP to perform tasks like mapping natural language instructions to robotic actions, generating situated robot task plans, or grounding language in robotic affordances.\nSome of the papers propose new models or methods that combine language, vision, and action for embodied reasoning, navigation, or control. For example, RT-1 is a robotics transformer that can learn from large-scale data and generalize to new environments. PaLM-E is an embodied multimodal language model that can interact with objects and agents in a 3D world. LLM+P is a method that empowers large language models with optimal planning proficiency.\nSome of the papers also provide codes, websites, or colabs for reproducing or testing their results. For example, you can try out Code-As-Policies, which uses language model programs for embodied control, or Socratic, which composes zero-shot multimodal reasoning with language.\n\n\nDiscussion points: how will the coding of robots change over time? What are the big outstanding problems?\n","metadata":{"execution":{}}},{"cell_type":"code","source":"# @title Submit your feedback\ncontent_review(f\"{feedback_prefix}_Language_for_Robotics_Discussion\")","metadata":{"cellView":"form","execution":{"iopub.status.busy":"2024-08-03T14:49:54.474204Z","iopub.execute_input":"2024-08-03T14:49:54.475014Z","iopub.status.idle":"2024-08-03T14:49:54.524500Z","shell.execute_reply.started":"2024-08-03T14:49:54.474973Z","shell.execute_reply":"2024-08-03T14:49:54.523466Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(VBox(children=(HBox(children=(Button(description='ðŸ™‚', layout=Layout(height='auto', padding='0.5â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5b22dfbed9f441aac888fc0bb10b2e0"}},"metadata":{}}]},{"cell_type":"markdown","source":"---\n# Summary\n\n*Time estimate: ~2 mins*\n\n\n\n","metadata":{"execution":{}}},{"cell_type":"markdown","source":"---\n# Daily survey\n\nDon't forget to complete your reflections and content check in the daily survey! Please be patient after logging in as there is a small delay before you will be redirected to the survey.\n\n\n<a href=\"https://portal.neuromatchacademy.org/api/redirect/to/95651a47-083f-406e-a6cd-8af24670c5bc\"><img src=\"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/SurveyButton.png?raw=1\" alt=\"button link to survey\" style=\"width:410px\"></a>","metadata":{"execution":{}}}]}